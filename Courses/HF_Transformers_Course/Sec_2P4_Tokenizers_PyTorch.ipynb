{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suresh-venkate/NLP_LLM/blob/main/Courses/HF_Transformers_Course/Sec_2P4_Tokenizers_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjrwCU-VWAyz"
      },
      "source": [
        "# HF Tokenizers -  PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfG7JN9jWAzA"
      },
      "source": [
        "## Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7gt7v6CZWAzB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Tokenizer"
      ],
      "metadata": {
        "id": "s3R1A0kVpdWW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IEI63GRwWAzG"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\") # Load the tokenizer algorithm used by 'bert-base-cased' along with its vocab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TwaqV6gJWAzH"
      },
      "outputs": [],
      "source": [
        "# Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name,\n",
        "# and can be used directly with any checkpoint:\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yj5gbLPIWAzI",
        "outputId": "308c7d16-5c75-4e04-fcb8-0bb7112569b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0_e8xiTBWAzJ",
        "outputId": "fcf4fb98-7965-4686-f528-1b7bd9e906c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('directory_on_my_computer/tokenizer_config.json',\n",
              " 'directory_on_my_computer/special_tokens_map.json',\n",
              " 'directory_on_my_computer/vocab.txt',\n",
              " 'directory_on_my_computer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization Pipeline\n",
        "\n",
        "\n",
        "\n",
        "1.   First step is to split the text into words (or part of words, puncutation symbols etc.) usually called tokens.\n",
        "2.   The second step is to convert the tokens into numbers, so that we can build a tensor out of them and feed them into the model. To do this, the tokenizer uses its vocabulary.\n",
        "\n"
      ],
      "metadata": {
        "id": "0VhchpTTqHNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization process"
      ],
      "metadata": {
        "id": "MsT_ka8tqpX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Y-pC56WWAzK",
        "outputId": "be3c2302-23df-4c8b-f6ca-6220cd3da7b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to ID's"
      ],
      "metadata": {
        "id": "rXqtTIA3qxYq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDEDpqIeWAzP",
        "outputId": "06980015-36c8-4979-fe65-d213582a49a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[7993, 170, 11303, 1200, 2443, 1110, 3014]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding ID's\n",
        "\n",
        "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode() method as follows:"
      ],
      "metadata": {
        "id": "xY1EihFwrPU-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYSOD9XGWAzP",
        "outputId": "0a1ad376-88a6-4506-a2a6-0ae718f3b963"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Using a Transformer network is simple'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization).\n"
      ],
      "metadata": {
        "id": "_428fO9nrcpc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tokenizers (PyTorch)",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}